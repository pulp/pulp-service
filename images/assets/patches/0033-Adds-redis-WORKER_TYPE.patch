From 5da34590481728f240fe08014864632342d6f9c2 Mon Sep 17 00:00:00 2001
From: Dennis Kliban <dkliban@redhat.com>
Date: Tue, 25 Nov 2025 11:06:48 -0500
Subject: [PATCH] Adds 'redis' WORKER_TYPE

This adds WORKER_TYPE setting. The default value is 'pulpcore'. When 'redis' is selected,
the tasking system uses Redis to lock resources. Redis workers produce less load on the
PostgreSQL database.
---
 pulpcore/app/models/task.py                   |  16 +-
 pulpcore/app/settings.py                      |   4 +
 pulpcore/app/tasks/test.py                    |  43 +
 pulpcore/app/viewsets/task.py                 |   6 +
 pulpcore/tasking/_util.py                     |   9 +-
 pulpcore/tasking/entrypoint.py                |  13 +-
 pulpcore/tasking/redis_locks.py               | 245 ++++++
 pulpcore/tasking/redis_tasks.py               | 482 +++++++++++
 pulpcore/tasking/redis_worker.py              | 777 ++++++++++++++++++
 pulpcore/tasking/tasks.py                     |  17 +
 pulpcore/tests/functional/api/test_tasking.py | 288 +++++++
 11 files changed, 1892 insertions(+), 8 deletions(-)
 create mode 100644 pulpcore/tasking/redis_locks.py
 create mode 100644 pulpcore/tasking/redis_tasks.py
 create mode 100644 pulpcore/tasking/redis_worker.py

diff --git a/pulpcore/app/models/task.py b/pulpcore/app/models/task.py
index ed0b2e87a..03c6e0be6 100644
--- a/pulpcore/app/models/task.py
+++ b/pulpcore/app/models/task.py
@@ -7,6 +7,7 @@ import logging
 import traceback
 from gettext import gettext as _
 
+from django.conf import settings
 from django.contrib.postgres.fields import ArrayField, HStoreField
 from django.contrib.postgres.indexes import GinIndex
 from django.core.serializers.json import DjangoJSONEncoder
@@ -207,13 +208,18 @@ class Task(BaseModel, AutoAddObjPermsMixin):
         This updates the :attr:`started_at` and sets the :attr:`state` to :attr:`RUNNING`.
         """
         started_at = timezone.now()
-        rows = Task.objects.filter(
-            pk=self.pk,
-            state=TASK_STATES.WAITING,
-            app_lock=AppStatus.objects.current(),
-        ).update(
+        filter_kwargs = {
+            "pk": self.pk,
+            "state": TASK_STATES.WAITING,
+        }
+        # Only check app_lock for PulpcoreWorker, not RedisWorker
+        if settings.WORKER_TYPE != "redis":
+            filter_kwargs["app_lock"] = AppStatus.objects.current()
+
+        rows = Task.objects.filter(**filter_kwargs).update(
             state=TASK_STATES.RUNNING,
             started_at=started_at,
+            app_lock=AppStatus.objects.current(),
         )
         if rows == 1:
             self.state = TASK_STATES.RUNNING
diff --git a/pulpcore/app/settings.py b/pulpcore/app/settings.py
index b6000c7da..5e4b2305e 100644
--- a/pulpcore/app/settings.py
+++ b/pulpcore/app/settings.py
@@ -293,6 +293,10 @@ API_APP_TTL = 120  # The heartbeat is called from gunicorn notify (defaulting to
 CONTENT_APP_TTL = 30
 WORKER_TTL = 30
 
+# Worker implementation type
+# Options: "pulpcore" (default, PostgreSQL advisory locks) or "redis" (Redis distributed locks)
+WORKER_TYPE = "pulpcore"
+
 # Seconds for a task to finish on semi graceful worker shutdown (approx)
 # On SIGHUP, SIGTERM the currently running task will be awaited forever.
 # On SIGINT, this value represents the time before the worker will attempt to kill the subprocess.
diff --git a/pulpcore/app/tasks/test.py b/pulpcore/app/tasks/test.py
index c52b9dc2a..7c0616ae8 100644
--- a/pulpcore/app/tasks/test.py
+++ b/pulpcore/app/tasks/test.py
@@ -1,5 +1,7 @@
 import asyncio
 import backoff
+import os
+import signal
 import time
 from pulpcore.app.models import TaskGroup
 from pulpcore.tasking.tasks import dispatch
@@ -32,3 +34,44 @@ def dummy_group_task(inbetween=3, intervals=None):
     for interval in intervals:
         dispatch(sleep, args=(interval,), task_group=task_group)
         time.sleep(inbetween)
+
+
+def missing_worker():
+    """
+    Simulates a worker crash by sending SIGKILL to parent process and itself.
+
+    This task is used for testing worker cleanup behavior when a worker
+    unexpectedly dies while executing a task.
+    """
+    parent_pid = os.getppid()
+    current_pid = os.getpid()
+
+    # Kill parent process (the worker)
+    os.kill(parent_pid, signal.SIGKILL)
+
+    # Kill current process (the task subprocess)
+    os.kill(current_pid, signal.SIGKILL)
+
+
+def failing_task(error_message="Task intentionally failed"):
+    """
+    A task that always raises a RuntimeError.
+
+    This task is used for testing error handling in worker task execution.
+
+    Args:
+        error_message (str): The error message to include in the RuntimeError
+    """
+    raise RuntimeError(error_message)
+
+
+async def afailing_task(error_message="Task intentionally failed"):
+    """
+    An async task that always raises a RuntimeError.
+
+    This task is used for testing error handling in immediate task execution.
+
+    Args:
+        error_message (str): The error message to include in the RuntimeError
+    """
+    raise RuntimeError(error_message)
diff --git a/pulpcore/app/viewsets/task.py b/pulpcore/app/viewsets/task.py
index a3f2f6d56..36c202385 100644
--- a/pulpcore/app/viewsets/task.py
+++ b/pulpcore/app/viewsets/task.py
@@ -1,5 +1,6 @@
 from gettext import gettext as _
 
+from django.conf import settings
 from django.db import transaction
 from django.db.models import Prefetch
 from django_filters.rest_framework import filters
@@ -223,6 +224,11 @@ class TaskViewSet(
         responses={200: TaskSerializer, 409: TaskSerializer},
     )
     def partial_update(self, request, pk=None, partial=True):
+        # RedisWorker does not support task cancellation
+        if settings.WORKER_TYPE == "redis":
+            return Response(status=status.HTTP_501_NOT_IMPLEMENTED)
+
+        # PulpcoreWorker supports task cancellation
         serializer = self.get_serializer(data=request.data)
         serializer.is_valid(raise_exception=True)
 
diff --git a/pulpcore/tasking/_util.py b/pulpcore/tasking/_util.py
index 49ecaecdb..341602053 100644
--- a/pulpcore/tasking/_util.py
+++ b/pulpcore/tasking/_util.py
@@ -23,7 +23,14 @@ from pulpcore.app.util import (
     configure_periodic_telemetry,
 )
 from pulpcore.constants import TASK_FINAL_STATES, TASK_STATES
-from pulpcore.tasking.tasks import dispatch, execute_task
+from pulpcore.tasking.tasks import dispatch
+
+# Conditionally import execute_task based on WORKER_TYPE
+if settings.WORKER_TYPE == "redis":
+    from pulpcore.tasking.redis_tasks import execute_task
+else:
+    from pulpcore.tasking.tasks import execute_task
+
 from pulp_service.app.tasks.util import (
     content_sources_periodic_telemetry,
     rhel_ai_repos_periodic_telemetry,
diff --git a/pulpcore/tasking/entrypoint.py b/pulpcore/tasking/entrypoint.py
index fdc9b0de8..a3eb7e01c 100644
--- a/pulpcore/tasking/entrypoint.py
+++ b/pulpcore/tasking/entrypoint.py
@@ -10,6 +10,7 @@ django.setup()
 
 from django.conf import settings  # noqa: E402: module level not at top
 from pulpcore.tasking.worker import PulpcoreWorker  # noqa: E402: module level not at top
+from pulpcore.tasking.redis_worker import RedisWorker  # noqa: E402: module level not at top
 
 
 _logger = logging.getLogger(__name__)
@@ -59,6 +60,14 @@ def worker(
     if name_template:
         settings.set("WORKER_NAME_TEMPLATE", name_template)
 
-    _logger.info("Starting distributed type worker")
+    worker_type = settings.WORKER_TYPE
+    _logger.info("Starting %s worker", worker_type)
 
-    PulpcoreWorker(auxiliary=auxiliary).run(burst=burst)
+    if worker_type == "redis":
+        if auxiliary:
+            _logger.warning("RedisWorker does not support auxiliary mode, ignoring --auxiliary flag")
+        RedisWorker().run(burst=burst)
+    elif worker_type == "pulpcore":
+        PulpcoreWorker(auxiliary=auxiliary).run(burst=burst)
+    else:
+        raise ValueError(f"Invalid WORKER_TYPE: {worker_type}. Must be 'pulpcore' or 'redis'.")
diff --git a/pulpcore/tasking/redis_locks.py b/pulpcore/tasking/redis_locks.py
new file mode 100644
index 000000000..84e8c6a4b
--- /dev/null
+++ b/pulpcore/tasking/redis_locks.py
@@ -0,0 +1,245 @@
+"""
+Redis distributed lock utilities for task resource coordination.
+
+This module provides functions and Lua scripts for managing exclusive and shared
+resource locks using Redis.
+"""
+
+import logging
+from asgiref.sync import sync_to_async
+
+from pulpcore.app.redis_connection import get_redis_connection
+
+
+_logger = logging.getLogger(__name__)
+
+# Redis key prefix for resource locks
+REDIS_LOCK_PREFIX = "pulp:resource_lock:"
+
+# Lua script for atomic lock release (only release if we own the lock)
+REDIS_UNLOCK_SCRIPT = """
+if redis.call("get", KEYS[1]) == ARGV[1] then
+    return redis.call("del", KEYS[1])
+else
+    return 0
+end
+"""
+
+REDIS_ACQUIRE_LOCKS_SCRIPT = """
+-- KEYS: [exclusive_lock_keys..., shared_lock_keys...]
+-- ARGV[1]: lock_owner (task identifier)
+-- ARGV[2]: number of exclusive resources
+-- ARGV[3...]: exclusive resource names (for error reporting)
+-- Returns: empty table if success, table of blocked exclusive resource names if failed
+
+local num_exclusive = tonumber(ARGV[2])
+local lock_owner = ARGV[1]
+local acquired_exclusive = {}
+local acquired_shared = {}
+local blocked_resources = {}
+
+-- Try to acquire exclusive locks
+for i = 1, num_exclusive do
+    local key = KEYS[i]
+    local resource_name = ARGV[2 + i]
+
+    -- Check if lock exists
+    if redis.call("exists", key) == 1 then
+        -- Lock already held, add to blocked list
+        table.insert(blocked_resources, resource_name)
+    end
+end
+
+-- If any exclusive locks were blocked, don't proceed
+if #blocked_resources > 0 then
+    return blocked_resources
+end
+
+-- Check shared resources - ensure no exclusive locks exist
+for i = num_exclusive + 1, #KEYS do
+    local key = KEYS[i]
+    local shared_resource_name = ARGV[2 + i]
+
+    -- Check if there's an exclusive lock (string value)
+    local lock_type = redis.call("type", key)
+    if lock_type["ok"] == "string" then
+        -- Exclusive lock exists on a shared resource we need
+        -- This counts as a blocked resource
+        table.insert(blocked_resources, shared_resource_name)
+    end
+end
+
+-- If any shared resources are blocked by exclusive locks, fail
+if #blocked_resources > 0 then
+    return blocked_resources
+end
+
+-- All checks passed, acquire the locks
+for i = 1, num_exclusive do
+    local key = KEYS[i]
+    redis.call("set", key, lock_owner)
+    table.insert(acquired_exclusive, key)
+end
+
+for i = num_exclusive + 1, #KEYS do
+    local key = KEYS[i]
+    redis.call("sadd", key, lock_owner)
+    table.insert(acquired_shared, key)
+end
+
+-- Return empty table to indicate success
+return {}
+"""
+
+
+def resource_to_lock_key(resource_name):
+    """
+    Convert a resource name to a Redis lock key.
+
+    Args:
+        resource_name (str): The resource name (e.g., "prn:rpm.repository:abc123")
+
+    Returns:
+        str: A Redis key for the resource lock
+    """
+    return f"{REDIS_LOCK_PREFIX}{resource_name}"
+
+
+def acquire_locks(redis_conn, lock_owner, exclusive_resources, shared_resources):
+    """
+    Atomically try to acquire exclusive and shared locks for resources.
+
+    Args:
+        redis_conn: Redis connection
+        lock_owner (str): The identifier of the lock owner (worker/task)
+        exclusive_resources (list): List of exclusive resource names
+        shared_resources (list): List of shared resource names
+
+    Returns:
+        list: Empty list if all locks acquired successfully,
+              list of blocked resource names if acquisition failed
+    """
+    if not redis_conn:
+        return []
+
+    # Sort resources deterministically to prevent deadlocks
+    exclusive_resources = sorted(exclusive_resources) if exclusive_resources else []
+    shared_resources = sorted(shared_resources) if shared_resources else []
+
+    if not exclusive_resources and not shared_resources:
+        return []
+
+    # Build KEYS list: exclusive lock keys + shared lock keys
+    keys = []
+    for resource in exclusive_resources:
+        keys.append(resource_to_lock_key(resource))
+    for resource in shared_resources:
+        keys.append(resource_to_lock_key(resource))
+
+    # Build ARGV list: lock_owner, num_exclusive, resource names (for error reporting)
+    args = [lock_owner, str(len(exclusive_resources))]
+    args.extend(exclusive_resources)
+    args.extend(shared_resources)
+
+    # Register and execute the Lua script
+    acquire_script = redis_conn.register_script(REDIS_ACQUIRE_LOCKS_SCRIPT)
+    try:
+        blocked_resources = acquire_script(keys=keys, args=args)
+        # Redis returns list of blocked resources or empty list
+        return blocked_resources if blocked_resources else []
+    except Exception as e:
+        _logger.error("Error acquiring locks: %s", e)
+        return ["error"]  # Return non-empty list to indicate failure
+
+
+def release_shared_resource_locks(redis_conn, lock_owner, shared_resources):
+    """
+    Release shared resource locks by removing task from Redis sets.
+
+    Args:
+        redis_conn: Redis connection
+        lock_owner (str): The identifier of the lock owner (task ID)
+        shared_resources (list): List of shared resource names
+    """
+    if not redis_conn or not shared_resources:
+        return
+
+    for resource in shared_resources:
+        try:
+            lock_key = resource_to_lock_key(resource)
+            # Remove this task from the shared resource set
+            removed = redis_conn.srem(lock_key, lock_owner)
+            if removed:
+                _logger.debug("Released shared resource: %s", resource)
+            else:
+                _logger.warning("Shared resource %s did not contain %s", resource, lock_owner)
+        except Exception as e:
+            _logger.error("Error releasing shared resource %s: %s", resource, e)
+
+
+def release_resource_locks(redis_conn, lock_owner, resources, shared_resources=None):
+    """
+    Release Redis distributed locks for exclusive and shared resources.
+
+    Uses a Lua script to ensure we only release exclusive locks that we own.
+    Removes task from shared resource sets.
+
+    Args:
+        redis_conn: Redis connection
+        lock_owner (str): The identifier of the lock owner
+        resources (list): List of exclusive resource names to release locks for
+        shared_resources (list): Optional list of shared resource names
+    """
+    if not redis_conn:
+        return
+
+    # Release exclusive locks
+    if resources:
+        # Register the unlock script
+        unlock_script = redis_conn.register_script(REDIS_UNLOCK_SCRIPT)
+
+        for resource in resources:
+            try:
+                lock_key = resource_to_lock_key(resource)
+                # Use Lua script to atomically check and delete only if we own the lock
+                released = unlock_script(keys=[lock_key], args=[lock_owner])
+                if released:
+                    _logger.debug("Released exclusive lock for resource: %s", resource)
+                else:
+                    _logger.warning("Lock for resource %s was not owned by %s", resource, lock_owner)
+            except Exception as e:
+                _logger.error("Error releasing lock for resource %s: %s", resource, e)
+
+    # Release shared resources
+    if shared_resources:
+        release_shared_resource_locks(redis_conn, lock_owner, shared_resources)
+
+
+async def async_release_resource_locks(redis_conn, lock_owner, resources):
+    """
+    Async version: Release Redis distributed locks for the given resources.
+
+    Uses a Lua script to ensure we only release locks that we own.
+
+    Args:
+        redis_conn: Redis connection
+        lock_owner (str): The identifier of the lock owner
+        resources (list): List of resource names to release locks for
+    """
+    if not redis_conn:
+        return
+
+    # Register the unlock script
+    unlock_script = await sync_to_async(redis_conn.register_script)(REDIS_UNLOCK_SCRIPT)
+
+    for resource in resources:
+        try:
+            lock_key = resource_to_lock_key(resource)
+            # Use Lua script to atomically check and delete only if we own the lock
+            released = await sync_to_async(unlock_script)(keys=[lock_key], args=[lock_owner])
+            if released:
+                _logger.debug("Released lock for resource: %s", resource)
+            else:
+                _logger.warning("Lock for resource %s was not owned by %s", resource, lock_owner)
+        except Exception as e:
+            _logger.error("Error releasing lock for resource %s: %s", resource, e)
diff --git a/pulpcore/tasking/redis_tasks.py b/pulpcore/tasking/redis_tasks.py
new file mode 100644
index 000000000..7f4396ba7
--- /dev/null
+++ b/pulpcore/tasking/redis_tasks.py
@@ -0,0 +1,482 @@
+"""
+Task dispatch functions for Redis-based worker implementation.
+
+This module contains dispatch logic specific to the Redis worker that uses
+Redis distributed locks for task coordination.
+"""
+
+import contextvars
+import logging
+import sys
+from asgiref.sync import sync_to_async
+
+from pulpcore.app.models import Task, AppStatus
+from pulpcore.app.redis_connection import get_redis_connection
+from pulpcore.app.util import get_domain
+from pulpcore.app.contexts import with_task_context, awith_task_context
+from pulpcore.constants import TASK_STATES
+from pulpcore.tasking.redis_locks import (
+    resource_to_lock_key,
+    release_resource_locks,
+    async_release_resource_locks,
+)
+from pulpcore.tasking.tasks import (
+    called_from_content_app,
+    get_function_name,
+    get_version,
+    get_resources,
+    get_task_payload,
+    get_task_function,
+    aget_task_function,
+    log_task_start,
+    log_task_completed,
+    log_task_failed,
+    using_workdir,
+)
+from pulpcore.tasking.kafka import send_task_notification
+
+
+_logger = logging.getLogger(__name__)
+
+
+def execute_task(task):
+    """Redis-aware task execution that releases Redis locks for immediate tasks."""
+    # This extra stack is needed to isolate the current_task ContextVar
+    contextvars.copy_context().run(_execute_task, task)
+
+
+def _execute_task(task):
+    try:
+        # Log execution context information
+        current_app = AppStatus.objects.current()
+        if current_app:
+            _logger.info(
+                "TASK EXECUTION: Task %s being executed by %s (app_type=%s)",
+                task.pk,
+                current_app.name,
+                current_app.app_type
+            )
+        else:
+            _logger.info(
+                "TASK EXECUTION: Task %s being executed with no AppStatus.current()",
+                task.pk
+            )
+
+        with with_task_context(task):
+            task.set_running()
+            # Verify task state was actually updated in database
+            db_task = Task.objects.get(pk=task.pk)
+            _logger.info(
+                "TASK STATE VERIFICATION: Task %s local state=%s, database state=%s",
+                task.pk,
+                task.state,
+                db_task.state
+            )
+            domain = get_domain()
+            try:
+                log_task_start(task, domain)
+                task_function = get_task_function(task)
+                result = task_function()
+            except Exception:
+                exc_type, exc, tb = sys.exc_info()
+                task.set_failed(exc, tb)
+                log_task_failed(task, exc_type, exc, tb, domain)
+                send_task_notification(task)
+            else:
+                task.set_completed(result)
+                log_task_completed(task, domain)
+                send_task_notification(task)
+                return result
+            return None
+    finally:
+        # Release Redis locks if this was an immediate task
+        if hasattr(task, '_locked_resources') and task._locked_resources:
+            current_app = AppStatus.objects.current()
+            redis_conn = get_redis_connection()
+            lock_owner = current_app.name if current_app else f"immediate-{task.pk}"
+            _logger.info(
+                "RESOURCE LOCK RELEASE: Task %s releasing resource locks with owner=%s (AppStatus.current=%s) for resources: %s",
+                task.pk,
+                lock_owner,
+                current_app.name if current_app else "None",
+                task._locked_resources
+            )
+            release_resource_locks(redis_conn, lock_owner, task._locked_resources)
+            # Clear the attribute so worker knows locks were released
+            del task._locked_resources
+
+
+async def aexecute_task(task):
+    """Redis-aware async task execution that releases Redis locks for immediate tasks."""
+    # This extra stack is needed to isolate the current_task ContextVar
+    await contextvars.copy_context().run(_aexecute_task, task)
+
+
+async def _aexecute_task(task):
+    try:
+        # Log execution context information
+        current_app = await sync_to_async(AppStatus.objects.current)()
+        if current_app:
+            _logger.info(
+                "TASK EXECUTION (async): Task %s being executed by %s (app_type=%s)",
+                task.pk,
+                current_app.name,
+                current_app.app_type
+            )
+        else:
+            _logger.info(
+                "TASK EXECUTION (async): Task %s being executed with no AppStatus.current()",
+                task.pk
+            )
+
+        async with awith_task_context(task):
+            await sync_to_async(task.set_running)()
+            # Verify task state was actually updated in database
+            db_task = await sync_to_async(Task.objects.get)(pk=task.pk)
+            _logger.info(
+                "TASK STATE VERIFICATION (async): Task %s local state=%s, database state=%s",
+                task.pk,
+                task.state,
+                db_task.state
+            )
+            domain = get_domain()
+            try:
+                task_coroutine_fn = await aget_task_function(task)
+                result = await task_coroutine_fn()
+            except Exception:
+                exc_type, exc, tb = sys.exc_info()
+                await sync_to_async(task.set_failed)(exc, tb)
+                log_task_failed(task, exc_type, exc, tb, domain)
+                send_task_notification(task)
+            else:
+                await sync_to_async(task.set_completed)(result)
+                send_task_notification(task)
+                log_task_completed(task, domain)
+                return result
+            return None
+    finally:
+        # Release Redis locks if this was an immediate task
+        if hasattr(task, '_locked_resources') and task._locked_resources:
+            current_app = await sync_to_async(AppStatus.objects.current)()
+            redis_conn = get_redis_connection()
+            lock_owner = current_app.name if current_app else f"immediate-{task.pk}"
+            _logger.info(
+                "RESOURCE LOCK RELEASE (async): Task %s releasing resource locks with owner=%s (AppStatus.current=%s) for resources: %s",
+                task.pk,
+                lock_owner,
+                current_app.name if current_app else "None",
+                task._locked_resources
+            )
+            await async_release_resource_locks(redis_conn, lock_owner, task._locked_resources)
+            # Clear the attribute so worker knows locks were released
+            del task._locked_resources
+
+
+def are_resources_available(colliding_resources, task: Task) -> bool:
+    """
+    Try to acquire Redis locks for the task's exclusive resources.
+
+    Returns True if all locks were acquired, False otherwise.
+    Stores acquired locks on the task object for later release.
+    """
+    redis_conn = get_redis_connection()
+    if not redis_conn:
+        _logger.error("Redis connection not available for immediate task locking")
+        return False
+
+    # Get exclusive resources (those not prefixed with "shared:")
+    exclusive_resources = [
+        resource
+        for resource in task.reserved_resources_record or []
+        if not resource.startswith("shared:")
+    ]
+
+    if not exclusive_resources:
+        # No exclusive resources, so locks are available
+        task._locked_resources = []
+        return True
+
+    # Sort resources deterministically to prevent deadlocks
+    sorted_resources = sorted(exclusive_resources)
+
+    # Use AppStatus.current() to get a worker identifier for the lock value
+    # For immediate tasks, we use a special identifier
+    current_app = AppStatus.objects.current()
+    lock_owner = current_app.name if current_app else f"immediate-{task.pk}"
+
+    try:
+        for resource in sorted_resources:
+            lock_key = resource_to_lock_key(resource)
+
+            # Try to acquire lock using SET with NX (only set if not exists)
+            acquired = redis_conn.set(lock_key, lock_owner, nx=True)
+
+            if not acquired:
+                _logger.debug(
+                    "Failed to acquire lock for immediate task %s resource: %s",
+                    task.pk,
+                    resource
+                )
+                # Release any locks we acquired so far
+                release_resource_locks(redis_conn, lock_owner, sorted_resources[:sorted_resources.index(resource)])
+                return False
+
+        # All locks acquired successfully, store them for later release
+        task._locked_resources = sorted_resources
+        _logger.debug("Successfully acquired all locks for immediate task %s", task.pk)
+        return True
+
+    except Exception as e:
+        _logger.error("Error acquiring locks for immediate task %s: %s", task.pk, e)
+        # Try to release any locks we may have acquired
+        release_resource_locks(redis_conn, lock_owner, sorted_resources)
+        return False
+
+
+async def async_are_resources_available(colliding_resources, task: Task) -> bool:
+    """
+    Try to acquire Redis locks for the task's exclusive resources.
+
+    Returns True if all locks were acquired, False otherwise.
+    Stores acquired locks on the task object for later release.
+    """
+    redis_conn = get_redis_connection()
+    if not redis_conn:
+        _logger.error("Redis connection not available for immediate task locking")
+        return False
+
+    # Get exclusive resources (those not prefixed with "shared:")
+    exclusive_resources = [
+        resource
+        for resource in task.reserved_resources_record or []
+        if not resource.startswith("shared:")
+    ]
+
+    if not exclusive_resources:
+        # No exclusive resources, so locks are available
+        task._locked_resources = []
+        return True
+
+    # Sort resources deterministically to prevent deadlocks
+    sorted_resources = sorted(exclusive_resources)
+
+    # Use AppStatus.current() to get a worker identifier for the lock value
+    # For immediate tasks, we use a special identifier
+    current_app = AppStatus.objects.current()
+    lock_owner = current_app.name if current_app else f"immediate-{task.pk}"
+
+    try:
+        for resource in sorted_resources:
+            lock_key = resource_to_lock_key(resource)
+
+            # Try to acquire lock using SET with NX (only set if not exists)
+            acquired = await sync_to_async(redis_conn.set)(lock_key, lock_owner, nx=True)
+
+            if not acquired:
+                _logger.debug(
+                    "Failed to acquire lock for immediate task %s resource: %s",
+                    task.pk,
+                    resource
+                )
+                # Release any locks we acquired so far
+                await async_release_resource_locks(redis_conn, lock_owner, sorted_resources[:sorted_resources.index(resource)])
+                return False
+
+        # All locks acquired successfully, store them for later release
+        task._locked_resources = sorted_resources
+        _logger.debug("Successfully acquired all locks for immediate task %s", task.pk)
+        return True
+
+    except Exception as e:
+        _logger.error("Error acquiring locks for immediate task %s: %s", task.pk, e)
+        # Try to release any locks we may have acquired
+        await async_release_resource_locks(redis_conn, lock_owner, sorted_resources)
+        return False
+
+
+def dispatch(
+    func,
+    args=None,
+    kwargs=None,
+    task_group=None,
+    exclusive_resources=None,
+    shared_resources=None,
+    immediate=False,
+    deferred=True,
+    versions=None,
+):
+    """
+    Enqueue a message to Pulp workers with Redis-based resource locking.
+
+    This version uses Redis distributed locks instead of PostgreSQL advisory locks.
+
+    Args:
+        func (callable | str): The function to be run when the necessary locks are acquired.
+        args (tuple): The positional arguments to pass on to the task.
+        kwargs (dict): The keyword arguments to pass on to the task.
+        task_group (pulpcore.app.models.TaskGroup): A TaskGroup to add the created Task to.
+        exclusive_resources (list): A list of resources this task needs exclusive access to while
+            running. Each resource can be either a `str` or a `django.models.Model` instance.
+        shared_resources (list): A list of resources this task needs non-exclusive access to while
+            running. Each resource can be either a `str` or a `django.models.Model` instance.
+        immediate (bool): Whether to allow running this task immediately. It must be guaranteed to
+            execute fast without blocking. If not all resource constraints are met, the task will
+            either be returned in a canceled state or, if `deferred` is `True` be left in the queue
+            to be picked up by a worker eventually. Defaults to `False`.
+        deferred (bool): Whether to allow defer running the task to a pulpcore_worker. Defaults to
+            `True`. `immediate` and `deferred` cannot both be `False`.
+        versions (Optional[Dict[str, str]]): Minimum versions of components by app_label the worker
+            must provide to handle the task.
+
+    Returns (pulpcore.app.models.Task): The Pulp Task that was created.
+
+    Raises:
+        ValueError: When `resources` is an unsupported type.
+    """
+
+    execute_now = immediate and not called_from_content_app()
+    assert deferred or immediate, "A task must be at least `deferred` or `immediate`."
+    function_name = get_function_name(func)
+    versions = get_version(versions, function_name)
+    colliding_resources, resources = get_resources(exclusive_resources, shared_resources, immediate)
+    app_lock = None if not execute_now else AppStatus.objects.current()  # Lazy evaluation...
+    task_payload = get_task_payload(
+        function_name, task_group, args, kwargs, resources, versions, immediate, deferred, app_lock
+    )
+    task = Task.objects.create(**task_payload)
+    task.refresh_from_db()  # The database will have assigned a timestamp for us.
+    if execute_now:
+        # Try to acquire Redis task lock to prevent workers from picking up this task
+        redis_conn = get_redis_connection()
+        task_lock_key = f"task:{task.pk}"
+        current_app = AppStatus.objects.current()
+        lock_owner = current_app.name if current_app else f"immediate-{task.pk}"
+
+        # Use SET with NX (only set if not exists) and EX (expiration in seconds)
+        # 24 hours = 86400 seconds
+        task_lock_acquired = redis_conn.set(task_lock_key, lock_owner, nx=True, ex=86400)
+
+        if task_lock_acquired:
+            # Try to acquire resource locks
+            if are_resources_available(colliding_resources, task):
+                try:
+                    _logger.info(
+                        "IMMEDIATE DISPATCH: Task %s acquired task lock and resources available, executing immediately in API process (AppStatus.current=%s)",
+                        task.pk,
+                        lock_owner
+                    )
+                    with using_workdir():
+                        execute_task(task)
+                except Exception:
+                    # Exception before execute_task() completed
+                    # Release locks if they weren't already released by _execute_task()
+                    if hasattr(task, '_locked_resources') and task._locked_resources:
+                        redis_conn = get_redis_connection()
+                        release_resource_locks(redis_conn, lock_owner, task._locked_resources)
+                        del task._locked_resources
+                        # Also release task lock since we couldn't complete execution
+                        redis_conn.delete(task_lock_key)
+                    raise
+            elif deferred:
+                # Resources not available, release task lock and defer to worker
+                redis_conn.delete(task_lock_key)
+                _logger.info(
+                    "IMMEDIATE DISPATCH: Task %s resources not available, released task lock and deferring to worker",
+                    task.pk
+                )
+            else:
+                # Resources not available and can't be deferred
+                redis_conn.delete(task_lock_key)
+                task.set_canceling()
+                task.set_canceled(TASK_STATES.CANCELED, "Resources temporarily unavailable.")
+        elif deferred:
+            # Another process acquired the task lock, defer to worker
+            _logger.info(
+                "IMMEDIATE DISPATCH: Task %s could not acquire task lock, deferring to worker",
+                task.pk
+            )
+        else:
+            # Can't acquire task lock and can't be deferred
+            task.set_canceling()
+            task.set_canceled(TASK_STATES.CANCELED, "Resources temporarily unavailable.")
+    return task
+
+
+async def adispatch(
+    func,
+    args=None,
+    kwargs=None,
+    task_group=None,
+    exclusive_resources=None,
+    shared_resources=None,
+    immediate=False,
+    deferred=True,
+    versions=None,
+):
+    """Async version of Redis-based dispatch."""
+    execute_now = immediate and not called_from_content_app()
+    assert deferred or immediate, "A task must be at least `deferred` or `immediate`."
+    function_name = get_function_name(func)
+    versions = get_version(versions, function_name)
+    colliding_resources, resources = get_resources(exclusive_resources, shared_resources, immediate)
+    app_lock = None if not execute_now else AppStatus.objects.current()  # Lazy evaluation...
+    task_payload = get_task_payload(
+        function_name, task_group, args, kwargs, resources, versions, immediate, deferred, app_lock
+    )
+    task = await Task.objects.acreate(**task_payload)
+    await task.arefresh_from_db()  # The database will have assigned a timestamp for us.
+    if execute_now:
+        # Try to acquire Redis task lock to prevent workers from picking up this task
+        redis_conn = get_redis_connection()
+        task_lock_key = f"task:{task.pk}"
+        current_app = await sync_to_async(AppStatus.objects.current)()
+        lock_owner = current_app.name if current_app else f"immediate-{task.pk}"
+
+        # Use SET with NX (only set if not exists) and EX (expiration in seconds)
+        # 24 hours = 86400 seconds
+        task_lock_acquired = redis_conn.set(task_lock_key, lock_owner, nx=True, ex=86400)
+
+        if task_lock_acquired:
+            # Try to acquire resource locks
+            if await async_are_resources_available(colliding_resources, task):
+                try:
+                    _logger.info(
+                        "IMMEDIATE DISPATCH (async): Task %s acquired task lock and resources available, executing immediately in API process (AppStatus.current=%s)",
+                        task.pk,
+                        lock_owner
+                    )
+                    with using_workdir():
+                        await aexecute_task(task)
+                except Exception:
+                    # Exception before aexecute_task() completed
+                    # Release locks if they weren't already released by _aexecute_task()
+                    if hasattr(task, '_locked_resources') and task._locked_resources:
+                        redis_conn = get_redis_connection()
+                        await async_release_resource_locks(redis_conn, lock_owner, task._locked_resources)
+                        del task._locked_resources
+                        # Also release task lock since we couldn't complete execution
+                        redis_conn.delete(task_lock_key)
+                    raise
+            elif deferred:
+                # Resources not available, release task lock and defer to worker
+                redis_conn.delete(task_lock_key)
+                _logger.info(
+                    "IMMEDIATE DISPATCH (async): Task %s resources not available, released task lock and deferring to worker",
+                    task.pk
+                )
+            else:
+                # Resources not available and can't be deferred
+                redis_conn.delete(task_lock_key)
+                task.set_canceling()
+                task.set_canceled(TASK_STATES.CANCELED, "Resources temporarily unavailable.")
+        elif deferred:
+            # Another process acquired the task lock, defer to worker
+            _logger.info(
+                "IMMEDIATE DISPATCH (async): Task %s could not acquire task lock, deferring to worker",
+                task.pk
+            )
+        else:
+            # Can't acquire task lock and can't be deferred
+            task.set_canceling()
+            task.set_canceled(TASK_STATES.CANCELED, "Resources temporarily unavailable.")
+    return task
diff --git a/pulpcore/tasking/redis_worker.py b/pulpcore/tasking/redis_worker.py
new file mode 100644
index 000000000..13981bdd3
--- /dev/null
+++ b/pulpcore/tasking/redis_worker.py
@@ -0,0 +1,777 @@
+"""
+Redis-based worker implementation using distributed lock-based task fetching.
+
+This implementation uses a fundamentally different algorithm where workers compete
+directly for task resources using Redis distributed locks, eliminating the need
+for the unblocking mechanism and all task cancellation support.
+"""
+
+from gettext import gettext as _
+import functools
+import logging
+import os
+import random
+import select
+import signal
+import time
+from datetime import timedelta
+from multiprocessing import Process
+from tempfile import TemporaryDirectory
+
+from django.conf import settings
+from django.db import connection, transaction, DatabaseError, IntegrityError
+from django.utils import timezone
+
+from pulpcore.constants import (
+    TASK_STATES,
+    TASK_INCOMPLETE_STATES,
+    TASK_FINAL_STATES,
+    TASK_SCHEDULING_LOCK,
+    WORKER_CLEANUP_LOCK,
+    TASK_METRICS_LOCK,
+)
+from pulpcore.metrics import init_otel_meter
+from pulpcore.app.apps import pulp_plugin_configs
+from pulpcore.app.util import get_worker_name
+from pulpcore.app.models import Task, AppStatus
+from pulpcore.app.redis_connection import get_redis_connection
+from pulpcore.tasking.storage import WorkerDirectory
+from pulpcore.tasking._util import (
+    dispatch_scheduled_tasks,
+    perform_task,
+    startup_hook,
+)
+from pulpcore.tasking.redis_locks import (
+    resource_to_lock_key,
+    release_resource_locks,
+    acquire_locks,
+)
+from pulpcore.tasking.tasks import using_workdir
+from pulpcore.tasking.redis_tasks import execute_task
+
+
+_logger = logging.getLogger(__name__)
+random.seed()
+
+# Seconds for a task to finish on semi graceful worker shutdown (approx)
+TASK_GRACE_INTERVAL = settings.TASK_GRACE_INTERVAL
+# Seconds between attempts to kill the subprocess (approx)
+TASK_KILL_INTERVAL = 1
+# Number of heartbeats between cleaning up worker processes
+WORKER_CLEANUP_INTERVAL = 50
+# Number of heartbeats between rechecking ignored tasks
+IGNORED_TASKS_CLEANUP_INTERVAL = 100
+# Number of heartbeats between recording metrics
+METRIC_HEARTBEAT_INTERVAL = 3
+# Number of tasks to fetch in each query
+FETCH_TASK_LIMIT = 20
+
+
+def exclusive(lock):
+    """
+    Runs function in a transaction holding the specified lock.
+    Returns None if the lock could not be acquired.
+    It should be used for actions that only need to be performed by a single worker.
+    """
+
+    def _decorator(f):
+        @functools.wraps(f)
+        def _f(self, *args, **kwargs):
+            with transaction.atomic():
+                with connection.cursor() as cursor:
+                    cursor.execute("SELECT pg_try_advisory_xact_lock(%s, %s)", [0, lock])
+                    acquired = cursor.fetchone()[0]
+                if acquired:
+                    return f(self, *args, **kwargs)
+                else:
+                    return None
+
+        return _f
+
+    return _decorator
+
+
+class RedisWorker:
+    """
+    Worker implementation using Redis distributed lock-based resource acquisition.
+
+    This worker uses a simpler algorithm where:
+    1. Query waiting tasks (sorted by creation time, limited)
+    2. For each task, try to acquire Redis distributed locks for all resources
+    3. If all locks acquired, claim the task
+    4. Process resources in deterministic (sorted) order to prevent deadlocks
+    5. Lock values contain worker names to enable cleanup of stale locks
+
+    Note: This implementation does NOT support task cancellation.
+    """
+
+    def __init__(self):
+        # Notification states from signal handlers
+        self.shutdown_requested = False
+        self.wakeup_handle = False
+
+        self.ignored_task_ids = []
+        self.ignored_task_countdown = IGNORED_TASKS_CLEANUP_INTERVAL
+
+        self.task = None
+        self.name = get_worker_name()
+        self.heartbeat_period = timedelta(seconds=settings.WORKER_TTL / 3)
+        self.versions = {app.label: app.version for app in pulp_plugin_configs()}
+        self.app_status = AppStatus.objects.create(
+            name=self.name, app_type="worker", versions=self.versions
+        )
+
+        # This defaults to immediate task cancellation.
+        # It will be set into the future on moderately graceful worker shutdown,
+        # and set to None for fully graceful shutdown.
+        self.task_grace_timeout = timezone.now()
+
+        self.worker_cleanup_countdown = random.randint(
+            int(WORKER_CLEANUP_INTERVAL / 10), WORKER_CLEANUP_INTERVAL
+        )
+
+        # Metric recording interval
+        self.metric_heartbeat_countdown = METRIC_HEARTBEAT_INTERVAL
+
+        # Cache worker count for sleep calculation (updated during beat)
+        self.num_workers = 1
+
+        # Redis connection for distributed locks
+        self.redis_conn = get_redis_connection()
+
+        # Add a file descriptor to trigger select on signals
+        self.sentinel, sentinel_w = os.pipe()
+        os.set_blocking(self.sentinel, False)
+        os.set_blocking(sentinel_w, False)
+        signal.set_wakeup_fd(sentinel_w)
+
+        self._init_instrumentation()
+
+        startup_hook()
+
+        _logger.info("Initialized RedisWorker with Redis lock-based algorithm")
+
+    def _init_instrumentation(self):
+        """Initialize OpenTelemetry instrumentation if enabled."""
+        if settings.OTEL_ENABLED:
+            meter = init_otel_meter("pulp-worker")
+            self.waiting_tasks_meter = meter.create_gauge(
+                name="waiting_tasks",
+                description="Number of waiting and running tasks minus the number of workers.",
+                unit="tasks",
+            )
+            self.otel_enabled = True
+        else:
+            self.otel_enabled = False
+
+    def _signal_handler(self, thesignal, frame):
+        """Handle shutdown signals."""
+        if thesignal in (signal.SIGHUP, signal.SIGTERM):
+            _logger.info(_("Worker %s was requested to shut down gracefully."), self.name)
+            # Wait forever...
+            self.task_grace_timeout = None
+        else:
+            # Reset signal handlers to default
+            # If you kill the process a second time it's not graceful anymore.
+            signal.signal(signal.SIGINT, signal.SIG_DFL)
+            signal.signal(signal.SIGTERM, signal.SIG_DFL)
+            signal.signal(signal.SIGHUP, signal.SIG_DFL)
+
+            _logger.info(_("Worker %s was requested to shut down."), self.name)
+            self.task_grace_timeout = timezone.now() + timezone.timedelta(
+                seconds=TASK_GRACE_INTERVAL
+            )
+        self.shutdown_requested = True
+
+    def shutdown(self):
+        """Cleanup worker on shutdown."""
+        self.app_status.delete()
+        _logger.info(_("Worker %s was shut down."), self.name)
+
+    def handle_worker_heartbeat(self):
+        """
+        Update worker heartbeat records.
+
+        If the update fails (the record was deleted, the database is unreachable, ...) the worker
+        is shut down.
+        """
+        msg = "Worker heartbeat from '{name}' at time {timestamp}".format(
+            timestamp=self.app_status.last_heartbeat, name=self.name
+        )
+        try:
+            self.app_status.save_heartbeat()
+            _logger.debug(msg)
+        except (IntegrityError, DatabaseError):
+            _logger.error(f"Updating the heartbeat of worker {self.name} failed.")
+            self.shutdown_requested = True
+
+    def cleanup_ignored_tasks(self):
+        """Remove tasks from ignored list that are no longer incomplete."""
+        for pk in (
+            Task.objects.filter(pk__in=self.ignored_task_ids)
+            .exclude(state__in=TASK_INCOMPLETE_STATES)
+            .values_list("pk", flat=True)
+        ):
+            self.ignored_task_ids.remove(pk)
+
+    def cleanup_redis_locks_for_worker(self, worker_name):
+        """
+        Clean up Redis locks held by a specific worker and fail its tasks.
+
+        This is called when a worker is detected as missing to:
+        1. Find tasks locked by the worker (via task locks)
+        2. Mark those tasks as FAILED if not already in a final state
+        3. Release the task's exclusive resource locks
+        4. Delete the task lock
+
+        Args:
+            worker_name (str): Name of the missing worker
+        """
+        if not self.redis_conn:
+            return
+
+        try:
+            # Find task locks held by this worker
+            task_lock_pattern = "task:*"
+            tasks_failed = 0
+
+            for key in self.redis_conn.scan_iter(match=task_lock_pattern, count=100):
+                # Check if this task lock is held by the missing worker
+                lock_holder = self.redis_conn.get(key)
+                if lock_holder and lock_holder.decode('utf-8') == worker_name:
+                    # Extract task UUID from key (format: "task:{uuid}")
+                    task_uuid = key.decode('utf-8').split(':', 1)[1]
+
+                    try:
+                        # Load the task
+                        task = Task.objects.select_related('pulp_domain').get(pk=task_uuid)
+
+                        # Extract exclusive resources from the task
+                        exclusive_resources = [
+                            resource
+                            for resource in task.reserved_resources_record or []
+                            if not resource.startswith("shared:")
+                        ]
+
+                        # Release the resource locks
+                        # Note: Use the missing worker's name as the lock owner, not self.name
+                        if exclusive_resources:
+                            release_resource_locks(self.redis_conn, worker_name, exclusive_resources)
+                            _logger.info(
+                                "Released %d resource locks for task %s from missing worker %s",
+                                len(exclusive_resources),
+                                task_uuid,
+                                worker_name
+                            )
+
+                        # Mark task as failed if it's not already in a final state
+                        if task.state not in TASK_FINAL_STATES:
+                            error_msg = f"Task failed because worker {worker_name} went missing"
+                            task.set_failed(RuntimeError(error_msg), None)
+                            tasks_failed += 1
+                            _logger.warning(
+                                "Marked task %s (state=%s) as FAILED (was being executed by missing worker %s)",
+                                task_uuid,
+                                task.state,
+                                worker_name
+                            )
+                    except Task.DoesNotExist:
+                        _logger.warning(
+                            "Task %s locked by missing worker %s not found in database",
+                            task_uuid,
+                            worker_name
+                        )
+
+                    # Delete the task lock
+                    self.redis_conn.delete(key)
+
+            if tasks_failed > 0:
+                _logger.info(
+                    "Cleanup for missing worker %s: failed %d tasks",
+                    worker_name,
+                    tasks_failed
+                )
+        except Exception as e:
+            _logger.error("Error cleaning up locks for worker %s: %s", worker_name, e)
+
+    @exclusive(WORKER_CLEANUP_LOCK)
+    def app_worker_cleanup(self):
+        """Cleanup records of missing app processes and their Redis locks."""
+        qs = AppStatus.objects.missing()
+        for app_worker in qs:
+            _logger.warning(
+                "Cleanup record of missing %s process %s.", app_worker.app_type, app_worker.name
+            )
+            # Clean up any Redis locks held by this missing process
+            # This includes workers and API processes (which can hold locks for immediate tasks)
+            self.cleanup_redis_locks_for_worker(app_worker.name)
+        qs.delete()
+
+    @exclusive(TASK_SCHEDULING_LOCK)
+    def dispatch_scheduled_tasks(self):
+        """Dispatch scheduled tasks."""
+        dispatch_scheduled_tasks()
+
+    @exclusive(TASK_METRICS_LOCK)
+    def record_waiting_tasks_metric(self):
+        """
+        Record metrics for waiting tasks in the queue.
+
+        This method counts all tasks in RUNNING or WAITING state that are older
+        than 5 seconds, then subtracts the number of active workers to get the
+        number of tasks waiting to be picked up by workers.
+        """
+        # Calculate the cutoff time (5 seconds ago)
+        cutoff_time = timezone.now() - timedelta(seconds=5)
+
+        # Count tasks in RUNNING or WAITING state older than 5 seconds
+        task_count = Task.objects.filter(
+            state__in=[TASK_STATES.RUNNING, TASK_STATES.WAITING],
+            pulp_created__lt=cutoff_time
+        ).count()
+
+        # Calculate waiting tasks: total tasks - workers
+        waiting_tasks = task_count - self.num_workers
+
+        # Set the metric value
+        self.waiting_tasks_meter.set(waiting_tasks)
+
+        _logger.debug(
+            "Waiting tasks metric: %d tasks (%d total tasks older than 5s - %d workers)",
+            waiting_tasks,
+            task_count,
+            self.num_workers
+        )
+
+    def beat(self):
+        """Periodic worker maintenance tasks (heartbeat, cleanup, etc.)."""
+        now = timezone.now()
+        if self.app_status.last_heartbeat < now - self.heartbeat_period:
+            self.handle_worker_heartbeat()
+            if self.ignored_task_ids:
+                self.ignored_task_countdown -= 1
+                if self.ignored_task_countdown <= 0:
+                    self.ignored_task_countdown = IGNORED_TASKS_CLEANUP_INTERVAL
+                    self.cleanup_ignored_tasks()
+
+            self.worker_cleanup_countdown -= 1
+            if self.worker_cleanup_countdown <= 0:
+                self.worker_cleanup_countdown = WORKER_CLEANUP_INTERVAL
+                self.app_worker_cleanup()
+
+            self.dispatch_scheduled_tasks()
+
+            # Record metrics periodically
+            if self.otel_enabled:
+                self.metric_heartbeat_countdown -= 1
+                if self.metric_heartbeat_countdown <= 0:
+                    self.metric_heartbeat_countdown = METRIC_HEARTBEAT_INTERVAL
+                    self.record_waiting_tasks_metric()
+
+            # Update cached worker count for sleep calculation
+            self.num_workers = AppStatus.objects.online().filter(app_type='worker').count()
+
+    def _try_acquire_resource_locks(self, resources):
+        """
+        Try to acquire Redis distributed locks for all resources in deterministic order.
+
+        The lock value is set to the worker name, which allows the cleanup code to
+        identify and remove locks from missing workers.
+
+        Args:
+            resources (list): List of resource names
+
+        Returns:
+            tuple: (success: bool, blocked_resources: list)
+                - If all locks acquired: (True, [])
+                - If a lock failed: (False, [resource_name])
+        """
+        if not resources:
+            # No resources means no locks needed
+            return (True, [])
+
+        if not self.redis_conn:
+            _logger.error("Redis connection not available for locking")
+            return (False, [])
+
+        # Sort resources deterministically to prevent deadlocks
+        sorted_resources = sorted(resources)
+
+        try:
+            for resource in sorted_resources:
+                lock_key = resource_to_lock_key(resource)
+
+                # Try to acquire lock using SET with NX (only set if not exists)
+                # The value is the worker name, so cleanup can identify stale locks
+                acquired = self.redis_conn.set(lock_key, self.name, nx=True)
+
+                if not acquired:
+                    _logger.debug(
+                        "Failed to acquire lock for resource: %s (key: %s)",
+                        resource,
+                        lock_key
+                    )
+                    # Release any locks we acquired so far
+                    self._release_resource_locks(sorted_resources[:sorted_resources.index(resource)])
+                    return (False, [resource])
+
+                _logger.debug("Acquired lock for resource: %s", resource)
+
+            # All locks acquired successfully
+            _logger.debug("Successfully acquired all locks for %d resources", len(resources))
+            return (True, [])
+
+        except Exception as e:
+            _logger.error("Error acquiring locks: %s", e)
+            # Try to release any locks we may have acquired
+            self._release_resource_locks(sorted_resources)
+            return (False, [])
+
+    def _release_resource_locks(self, resources, shared_resources=None):
+        """
+        Release Redis distributed locks for exclusive and shared resources.
+
+        Uses a Lua script to ensure we only release locks that we own.
+
+        Args:
+            resources (list): List of exclusive resource names to release locks for
+            shared_resources (list): Optional list of shared resource names
+        """
+        release_resource_locks(self.redis_conn, self.name, resources, shared_resources)
+
+    def is_compatible(self, task):
+        """
+        Check if this worker is compatible with the task's version requirements.
+
+        Args:
+            task: Task object
+
+        Returns:
+            bool: True if compatible, False otherwise
+        """
+        from packaging.version import parse as parse_version
+
+        unmatched_versions = [
+            f"task: {label}>={version} worker: {self.versions.get(label)}"
+            for label, version in task.versions.items()
+            if label not in self.versions
+            or parse_version(self.versions[label]) < parse_version(version)
+        ]
+        if unmatched_versions:
+            domain = task.pulp_domain
+            _logger.info(
+                _("Incompatible versions to execute task %s in domain: %s by worker %s: %s"),
+                task.pk,
+                domain.name,
+                self.name,
+                ",".join(unmatched_versions),
+            )
+            return False
+        return True
+
+    def fetch_task(self):
+        """
+        Fetch an available waiting task using Redis locks.
+
+        This method:
+        1. Queries waiting tasks (sorted by creation time, limited)
+        2. For each task, attempts to acquire Redis distributed locks for exclusive resources
+        3. If resource locks acquired, attempts to claim the task with a Redis task lock (24h expiration)
+        4. Returns the first task for which both locks can be acquired
+
+        Returns:
+            Task: A task object if one was successfully locked, None otherwise
+        """
+        # Query waiting tasks, sorted by creation time, limited
+        waiting_tasks = Task.objects.filter(
+            state=TASK_STATES.WAITING
+        ).exclude(
+            pk__in=self.ignored_task_ids
+        ).order_by('pulp_created').select_related('pulp_domain')[:FETCH_TASK_LIMIT]
+
+        # Track resources that are blocked during this iteration
+        # If we find a resource is blocked, skip all tasks needing that resource
+        blocked_resources = set()
+
+        # Try to acquire locks for each task
+        for task in waiting_tasks:
+            try:
+                reserved_resources_record = task.reserved_resources_record or []
+
+                # Extract exclusive resources (non-shared)
+                exclusive_resources = [
+                    resource
+                    for resource in reserved_resources_record
+                    if not resource.startswith("shared:")
+                ]
+
+                # Extract shared resources (strip "shared:" prefix)
+                shared_resources = [
+                    resource[7:]  # Remove "shared:" prefix
+                    for resource in reserved_resources_record
+                    if resource.startswith("shared:")
+                ]
+
+                # Check if any of this task's resources are already known to be blocked
+                task_needs_blocked_resource = False
+                for resource in exclusive_resources:
+                    if resource in blocked_resources:
+                        task_needs_blocked_resource = True
+                        _logger.debug(
+                            "Task %s skipped: needs blocked resource %s",
+                            task.pk,
+                            resource
+                        )
+                        break
+                if not task_needs_blocked_resource:
+                    for resource in shared_resources:
+                        if resource in blocked_resources:
+                            task_needs_blocked_resource = True
+                            _logger.debug(
+                                "Task %s skipped: needs blocked resource %s",
+                                task.pk,
+                                resource
+                            )
+                            break
+
+                if task_needs_blocked_resource:
+                    # Skip this task, try next one
+                    continue
+
+                # First try to acquire task lock (lightweight single Redis operation)
+                # This prevents wasting time on resource locks if another worker already has the task
+                task_lock_key = f"task:{task.pk}"
+                # Use SET with NX (only set if not exists) and EX (expiration in seconds)
+                # 24 hours = 86400 seconds
+                task_lock_acquired = self.redis_conn.set(
+                    task_lock_key,
+                    self.name,
+                    nx=True,
+                    ex=86400
+                )
+
+                if task_lock_acquired:
+                    # Successfully claimed the task lock!
+                    _logger.info(
+                        "Worker %s acquired task lock for task %s in domain: %s",
+                        self.name,
+                        task.pk,
+                        task.pulp_domain.name
+                    )
+                    # Now try to acquire resource locks atomically using Lua script
+                    # This handles both exclusive and shared resources
+                    blocked_resource_list = acquire_locks(
+                        self.redis_conn,
+                        self.name,
+                        exclusive_resources,
+                        shared_resources
+                    )
+
+                    if not blocked_resource_list:
+                        # All locks acquired successfully!
+                        _logger.info(
+                            "Worker %s acquired all resources for task %s in domain: %s",
+                            self.name,
+                            task.pk,
+                            task.pulp_domain.name
+                        )
+                        # Store only exclusive resources for release later
+                        # Shared resources are in Redis sets and will be cleaned up separately
+                        task._locked_resources = exclusive_resources
+                        task._locked_shared_resources = shared_resources
+                        return task
+                    else:
+                        # Failed to acquire resource locks
+                        # Release the task lock since we can't execute this task
+                        self.redis_conn.delete(task_lock_key)
+                        _logger.debug(
+                            "Worker %s acquired task lock but failed to acquire resource locks for task %s (blocked: %s), released task lock",
+                            self.name,
+                            task.pk,
+                            blocked_resource_list
+                        )
+                        # Add blocked resources to the blocked set
+                        for resource in blocked_resource_list:
+                            blocked_resources.add(resource)
+                else:
+                    # Another worker has the task lock, skip this task
+                    _logger.debug(
+                        "Worker %s skipped task %s - another worker holds task lock",
+                        self.name,
+                        task.pk
+                    )
+                    continue
+
+            except Exception as e:
+                _logger.error("Error processing task %s: %s", task.pk, e)
+                continue
+
+        # No task could be locked
+        return None
+
+    def supervise_immediate_task(self, task):
+        """Call and supervise the immediate async task process.
+
+        This function must only be called while holding the lock for that task."""
+        self.task = task
+        _logger.info(
+            "WORKER IMMEDIATE EXECUTION: Worker %s executing immediate task %s in domain: %s",
+            self.name,
+            task.pk,
+            task.pulp_domain.name
+        )
+        with using_workdir():
+            execute_task(task)
+        self.task = None
+
+    def supervise_task(self, task):
+        """Call and supervise the task process while heart beating.
+
+        This function must only be called while holding the lock for that task.
+        Note: This version does not support task cancellation."""
+
+        self.task = task
+        domain = task.pulp_domain
+        _logger.info(
+            "WORKER DEFERRED EXECUTION: Worker %s executing deferred task %s in domain: %s",
+            self.name,
+            task.pk,
+            domain.name
+        )
+        with TemporaryDirectory(dir=".") as task_working_dir_rel_path:
+            task_process = Process(target=perform_task, args=(task.pk, task_working_dir_rel_path))
+            task_process.start()
+
+            # Heartbeat while waiting for task to complete
+            while task_process.is_alive():
+                # Wait for a short period or until process completes
+                r, w, x = select.select(
+                    [self.sentinel, task_process.sentinel],
+                    [],
+                    [],
+                    self.heartbeat_period.seconds,
+                )
+                # Call beat to keep worker heartbeat alive and perform periodic tasks
+                self.beat()
+
+                if self.sentinel in r:
+                    os.read(self.sentinel, 256)
+
+                if task_process.sentinel in r:
+                    if not task_process.is_alive():
+                        break
+
+                # If shutdown was requested, handle gracefully or abort
+                if self.shutdown_requested:
+                    if self.task_grace_timeout is None or self.task_grace_timeout > timezone.now():
+                        msg = (
+                            "Worker shutdown requested, waiting for task {pk} in domain: {name} "
+                            "to finish.".format(pk=task.pk, name=domain.name)
+                        )
+                        _logger.info(msg)
+                    else:
+                        _logger.info(
+                            "Aborting current task %s in domain: %s due to worker shutdown.",
+                            task.pk,
+                            domain.name,
+                        )
+                        # Send SIGUSR1 to task process to trigger graceful abort
+                        os.kill(task_process.pid, signal.SIGUSR1)
+                        self.task_grace_timeout = timezone.now() + timezone.timedelta(
+                            seconds=TASK_KILL_INTERVAL
+                        )
+
+            task_process.join()
+            if task_process.exitcode != 0:
+                _logger.warning(
+                    "Task process for %s exited with non zero exitcode %i.",
+                    task.pk,
+                    task_process.exitcode,
+                )
+        self.task = None
+
+    def handle_tasks(self):
+        """Pick and supervise tasks until there are no more available tasks."""
+        while not self.shutdown_requested:
+            task = None
+            try:
+                task = self.fetch_task()
+                if task is None:
+                    # No task found
+                    break
+
+                if not self.is_compatible(task):
+                    # Incompatible task, add to ignored list
+                    self.ignored_task_ids.append(task.pk)
+                    # Release both exclusive and shared resource locks since we're not executing this task
+                    exclusive_resources = getattr(task, '_locked_resources', [])
+                    shared_resources = getattr(task, '_locked_shared_resources', [])
+                    if exclusive_resources or shared_resources:
+                        self._release_resource_locks(exclusive_resources, shared_resources)
+                    # Release the task lock so other workers can attempt it
+                    task_lock_key = f"task:{task.pk}"
+                    self.redis_conn.delete(task_lock_key)
+                    break
+
+                # Task is compatible, execute it
+                if task.immediate:
+                    self.supervise_immediate_task(task)
+                else:
+                    self.supervise_task(task)
+            finally:
+                # If _execute_task() ran, it will have released resource locks and deleted
+                # the _locked_resources attribute. Only release if attributes still exist.
+                if task:
+                    exclusive_resources = getattr(task, '_locked_resources', None)
+                    shared_resources = getattr(task, '_locked_shared_resources', None)
+                    if exclusive_resources or shared_resources:
+                        self._release_resource_locks(
+                            exclusive_resources or [],
+                            shared_resources or []
+                        )
+
+
+    def sleep(self):
+        """Sleep while calling beat() to maintain heartbeat and perform periodic tasks.
+
+        Sleep time = (num_workers * 10ms) + random_jitter(0.5ms, 1.5ms)
+        """
+        # Calculate sleep time: (num_workers * 10ms) + jitter(0.5-1.5ms)
+        base_sleep_ms = self.num_workers * 10.0
+        jitter_ms = random.uniform(0.5, 1.5)
+        sleep_time_seconds = (base_sleep_ms + jitter_ms) / 1000.0
+
+        _logger.debug(
+            _("Worker %s sleeping for %.4f seconds (workers=%d)"),
+            self.name,
+            sleep_time_seconds,
+            self.num_workers
+        )
+
+        # Call beat before sleeping to maintain heartbeat and perform periodic tasks
+        self.beat()
+
+        time.sleep(sleep_time_seconds)
+
+    def run(self, burst=False):
+        """Main worker loop."""
+        with WorkerDirectory(self.name):
+            signal.signal(signal.SIGINT, self._signal_handler)
+            signal.signal(signal.SIGTERM, self._signal_handler)
+            signal.signal(signal.SIGHUP, self._signal_handler)
+
+            if burst:
+                # Burst mode: process tasks until none are available
+                self.handle_tasks()
+            else:
+                # Normal mode: loop and sleep when no tasks available
+                while not self.shutdown_requested:
+                    if self.shutdown_requested:
+                        break
+                    self.handle_tasks()
+                    if self.shutdown_requested:
+                        break
+                    # Sleep until work arrives or heartbeat needed
+                    self.sleep()
+
+            self.shutdown()
diff --git a/pulpcore/tasking/tasks.py b/pulpcore/tasking/tasks.py
index c187bd919..d0af31376 100644
--- a/pulpcore/tasking/tasks.py
+++ b/pulpcore/tasking/tasks.py
@@ -261,7 +261,15 @@ def dispatch(
     Raises:
         ValueError: When `resources` is an unsupported type.
     """
+    # Check WORKER_TYPE setting and delegate to appropriate implementation
+    if settings.WORKER_TYPE == "redis":
+        from pulpcore.tasking.redis_tasks import dispatch as redis_dispatch
+        return redis_dispatch(
+            func, args, kwargs, task_group, exclusive_resources, shared_resources,
+            immediate, deferred, versions
+        )
 
+    # Original pulpcore implementation using PostgreSQL advisory locks
     execute_now = immediate and not called_from_content_app()
     assert deferred or immediate, "A task must be at least `deferred` or `immediate`."
     send_wakeup_signal = not execute_now
@@ -303,6 +311,15 @@ async def adispatch(
     versions=None,
 ):
     """Async version of dispatch."""
+    # Check WORKER_TYPE setting and delegate to appropriate implementation
+    if settings.WORKER_TYPE == "redis":
+        from pulpcore.tasking.redis_tasks import adispatch as redis_adispatch
+        return await redis_adispatch(
+            func, args, kwargs, task_group, exclusive_resources, shared_resources,
+            immediate, deferred, versions
+        )
+
+    # Original pulpcore implementation using PostgreSQL advisory locks
     execute_now = immediate and not called_from_content_app()
     assert deferred or immediate, "A task must be at least `deferred` or `immediate`."
     function_name = get_function_name(func)
diff --git a/pulpcore/tests/functional/api/test_tasking.py b/pulpcore/tests/functional/api/test_tasking.py
index 183ec2f00..96d547a79 100644
--- a/pulpcore/tests/functional/api/test_tasking.py
+++ b/pulpcore/tests/functional/api/test_tasking.py
@@ -86,6 +86,194 @@ def test_multi_resource_locking(dispatch_task, monitor_task):
     assert task1.finished_at < task5.started_at
 
 
+@pytest.mark.parallel
+def test_comprehensive_multi_resource_locking(dispatch_task, monitor_task):
+    """
+    Comprehensive test of resource locking with many tasks to keep 10 workers busy.
+
+    This test creates ~40 tasks with various resource patterns:
+    - Wave 1: Tasks with different exclusive resources (run in parallel)
+    - Wave 2: Tasks with shared access (run in parallel)
+    - Wave 3: Tasks with exclusive access (must wait)
+    - Wave 4: Tasks with mixed resource combinations
+
+    Verifies that all tasks complete successfully and resource locking works correctly.
+    """
+    task_hrefs = []
+
+    # Wave 1: 10 tasks with different exclusive resources (can all run in parallel)
+    # These should all start immediately and run concurrently
+    resources_wave1 = [f"R{i}" for i in range(1, 11)]
+    for i, resource in enumerate(resources_wave1):
+        task_href = dispatch_task(
+            "pulpcore.app.tasks.test.sleep",
+            args=(2,),  # 2 second tasks
+            exclusive_resources=[resource]
+        )
+        task_hrefs.append(task_href)
+
+    # Wave 2: 10 tasks with shared access to Wave 1 resources
+    # These should wait for Wave 1 to finish (can't share while exclusive lock held)
+    for i, resource in enumerate(resources_wave1):
+        task_href = dispatch_task(
+            "pulpcore.app.tasks.test.sleep",
+            args=(1,),  # 1 second tasks
+            shared_resources=[resource]
+        )
+        task_hrefs.append(task_href)
+
+    # Wave 3: 5 pairs of tasks sharing resources (can run in parallel)
+    # Each pair shares a resource, pairs can run concurrently
+    for i in range(5):
+        resource = f"S{i}"
+        # Two tasks sharing the same resource
+        task_href1 = dispatch_task(
+            "pulpcore.app.tasks.test.sleep",
+            args=(1,),
+            shared_resources=[resource]
+        )
+        task_href2 = dispatch_task(
+            "pulpcore.app.tasks.test.sleep",
+            args=(1,),
+            shared_resources=[resource]
+        )
+        task_hrefs.append(task_href1)
+        task_hrefs.append(task_href2)
+
+    # Wave 4: Tasks with exclusive access to shared resources (must wait)
+    for i in range(5):
+        resource = f"S{i}"
+        task_href = dispatch_task(
+            "pulpcore.app.tasks.test.sleep",
+            args=(1,),
+            exclusive_resources=[resource]
+        )
+        task_hrefs.append(task_href)
+
+    # Wave 5: Tasks with multiple resources (complex dependencies)
+    task_href = dispatch_task(
+        "pulpcore.app.tasks.test.sleep",
+        args=(1,),
+        exclusive_resources=["R1", "R2"],
+        shared_resources=["R3"]
+    )
+    task_hrefs.append(task_href)
+
+    task_href = dispatch_task(
+        "pulpcore.app.tasks.test.sleep",
+        args=(1,),
+        exclusive_resources=["R4"],
+        shared_resources=["R5", "R6"]
+    )
+    task_hrefs.append(task_href)
+
+    task_href = dispatch_task(
+        "pulpcore.app.tasks.test.sleep",
+        args=(1,),
+        shared_resources=["R1", "R2", "R3", "R4", "R5"]
+    )
+    task_hrefs.append(task_href)
+
+    # Monitor all tasks - this will wait for them all to complete
+    tasks = [monitor_task(href) for href in task_hrefs]
+
+    # Verify all tasks completed successfully
+    for task in tasks:
+        assert task.state == "completed", f"Task {task.pulp_href} failed with state {task.state}"
+
+    # Verify Wave 1 tasks (indices 0-9) started before Wave 2 tasks (indices 10-19)
+    # Since Wave 2 needs shared access to resources held exclusively by Wave 1
+    for i in range(10):
+        wave1_task = tasks[i]
+        wave2_task = tasks[i + 10]
+        assert wave1_task.finished_at < wave2_task.started_at, \
+            f"Wave 2 task {i} should have started after Wave 1 task {i} finished"
+
+    # Verify that shared resource tasks (Wave 3, indices 20-29) can run in parallel
+    # by checking that at least some overlap in execution time
+    wave3_tasks = tasks[20:30]
+    # Find earliest start and latest finish
+    earliest_start = min(t.started_at for t in wave3_tasks)
+    latest_start = max(t.started_at for t in wave3_tasks)
+    # If tasks ran truly sequentially, latest_start would be >> earliest_start
+    # With parallel execution, many should start close together
+    # At least 5 tasks should have started within 3 seconds of the earliest
+    tasks_started_early = sum(
+        1 for t in wave3_tasks
+        if (t.started_at - earliest_start).total_seconds() < 3
+    )
+    assert tasks_started_early >= 5, \
+        f"Expected at least 5 Wave 3 tasks to start in parallel, only {tasks_started_early} did"
+
+    # Verify exclusive access to shared resources (Wave 4, indices 30-34)
+    # must wait for shared tasks to finish
+    for i in range(5):
+        shared_task1 = tasks[20 + i * 2]  # First shared task for S{i}
+        shared_task2 = tasks[20 + i * 2 + 1]  # Second shared task for S{i}
+        exclusive_task = tasks[30 + i]  # Exclusive task for S{i}
+
+        # Exclusive task must start after both shared tasks finish
+        assert shared_task1.finished_at < exclusive_task.started_at, \
+            f"Exclusive S{i} task should start after shared tasks finish"
+        assert shared_task2.finished_at < exclusive_task.started_at, \
+            f"Exclusive S{i} task should start after shared tasks finish"
+
+
+@pytest.mark.parallel
+def test_worker_cleanup_on_missing_worker(dispatch_task, monitor_task, pulpcore_bindings):
+    """
+    Test that when a worker dies unexpectedly while executing a task,
+    the worker cleanup process marks the task as failed and releases its locks,
+    allowing subsequent tasks requiring the same resource to execute.
+    """
+    # Use a unique resource identifier to avoid conflicts with other tests
+    resource = str(uuid4())
+
+    # Dispatch the missing_worker task that will kill its worker process
+    task_href1 = dispatch_task(
+        "pulpcore.app.tasks.test.missing_worker",
+        exclusive_resources=[resource]
+    )
+
+    # Wait for the task to start running and the worker to die
+    time.sleep(2)
+
+    # Dispatch a second task that requires the same resource
+    task_href2 = dispatch_task(
+        "pulpcore.app.tasks.test.sleep",
+        args=(1,),
+        exclusive_resources=[resource]
+    )
+
+    # Wait for worker cleanup to run (happens every ~100 heartbeats)
+    # In tests, this should happen relatively quickly
+    # Monitor both tasks - the first should be marked as failed,
+    # and the second should complete successfully
+    max_wait = 180  # Wait up to 180 seconds for cleanup
+    start_time = time.time()
+
+    task1 = None
+    task2 = None
+
+    while time.time() - start_time < max_wait:
+        task1 = pulpcore_bindings.TasksApi.read(task_href1)
+        task2 = pulpcore_bindings.TasksApi.read(task_href2)
+
+        # Check if task1 is failed and task2 is completed
+        if task1.state == "failed" and task2.state == "completed":
+            break
+
+        time.sleep(1)
+
+    # Verify task1 was marked as failed
+    assert task1.state == "failed", f"Task 1 should be failed but is {task1.state}"
+    assert "worker" in task1.error["description"].lower() and "missing" in task1.error["description"].lower(), \
+        f"Task 1 error should mention missing worker: {task1.error['description']}"
+
+    # Verify task2 completed successfully after locks were released
+    assert task2.state == "completed", f"Task 2 should be completed but is {task2.state}"
+
+
 @pytest.mark.parallel
 def test_delete_cancel_waiting_task(dispatch_task, pulpcore_bindings):
     # Queue one task after a long running one
@@ -487,6 +675,106 @@ class TestImmediateTaskWithNoResource:
         assert "timed out after" in task.error["description"]
 
 
+@pytest.mark.parallel
+def test_immediate_task_execution_in_worker(dispatch_task, monitor_task):
+    """
+    GIVEN an immediate async task marked as deferred
+    AND a resource is blocked by another task
+    WHEN the immediate task cannot execute in the API due to blocked resource
+    THEN the task is deferred to a worker and executes successfully
+
+    This test verifies that workers can correctly execute immediate async tasks
+    using aexecute_task() instead of execute_task().
+    """
+    # Use a unique resource to avoid conflicts with other tests
+    resource = str(uuid4())
+
+    # Dispatch a blocking task that holds the resource
+    blocking_task_href = dispatch_task(
+        "pulpcore.app.tasks.test.sleep",
+        args=(3,),  # Runs for 3 seconds
+        exclusive_resources=[resource]
+    )
+
+    # Dispatch the immediate task that needs the same resource
+    # Since the resource is blocked, API cannot execute it immediately
+    # It will be deferred to a worker
+    task_href = dispatch_task(
+        "pulpcore.app.tasks.test.asleep",
+        args=(0.1,),  # Short sleep to make test fast
+        immediate=True,
+        deferred=True,
+        exclusive_resources=[resource]
+    )
+
+    # Monitor the task - it should complete successfully after blocking task finishes
+    task = monitor_task(task_href)
+
+    # Verify task completed successfully
+    assert task.state == "completed", f"Task should be completed but is {task.state}"
+
+    # Verify state transitions occurred correctly
+    assert task.started_at is not None, "Task should have a started_at timestamp"
+    assert task.finished_at is not None, "Task should have a finished_at timestamp"
+    assert task.started_at < task.finished_at, "Task should have started before finishing"
+
+    # Verify there's no error
+    assert task.error is None, f"Task should not have an error but has: {task.error}"
+
+    # Clean up blocking task
+    monitor_task(blocking_task_href)
+
+
+@pytest.mark.parallel
+def test_failing_immediate_task_error_handling(dispatch_task, monitor_task):
+    """
+    GIVEN a task that raises a RuntimeError
+    AND the task is an async function
+    WHEN dispatching the task as immediate and deferred
+    THEN the task fails with the correct error message
+    AND the error field contains the exception details
+    """
+    custom_error_message = "This is a custom error message"
+    with pytest.raises(PulpTaskError) as ctx:
+        task_href = dispatch_task(
+            "pulpcore.app.tasks.test.afailing_task",
+            kwargs={"error_message": custom_error_message},
+            immediate=True,
+            deferred=True,
+        )
+        monitor_task(task_href)
+
+    task = ctx.value.task
+    assert task.state == "failed"
+    assert task.error is not None
+    assert "description" in task.error
+    assert custom_error_message in task.error["description"]
+
+
+@pytest.mark.parallel
+def test_failing_worker_task_error_handling(dispatch_task, monitor_task):
+    """
+    GIVEN a task that raises a RuntimeError
+    AND the task is a sync function
+    WHEN dispatching the task as deferred (executes on worker)
+    THEN the task fails with the correct error message
+    AND the error field contains the exception details
+    """
+    custom_error_message = "Worker task failed with custom error"
+    with pytest.raises(PulpTaskError) as ctx:
+        task_href = dispatch_task(
+            "pulpcore.app.tasks.test.failing_task",
+            kwargs={"error_message": custom_error_message},
+        )
+        monitor_task(task_href)
+
+    task = ctx.value.task
+    assert task.state == "failed"
+    assert task.error is not None
+    assert "description" in task.error
+    assert custom_error_message in task.error["description"]
+
+
 @pytest.fixture
 def resource_blocker(pulpcore_bindings, dispatch_task):
 
-- 
2.52.0

